{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999297cc-55a6-477a-a254-2983cc50e633",
   "metadata": {},
   "source": [
    "# alpaca.cpp + fastLLaMa\n",
    "\n",
    "- [Sosaka/Alpaca-native-4bit-ggml](https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml)\n",
    "- [PotatoSpudowski/fastLLaMa](https://github.com/PotatoSpudowski/fastLLaMa)\n",
    "\n",
    "```bash\n",
    "git clone https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml\n",
    "git clone https://github.com/PotatoSpudowski/fastLLaMa\n",
    "```\n",
    "\n",
    "## fastLLaMa\n",
    "\n",
    "```bash\n",
    "cd fastLLaMa\n",
    "pip install -r requirements.txt\n",
    "python setup.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860dddca-6865-4c19-b712-a07394ffd0fe",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313b7f32-643c-4ff3-9acf-973c3b873fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./fastLLaMa/build/\")\n",
    "\n",
    "import fastLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1393dbd7-f619-4f96-a542-b86a3fec8ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID=\"ALPACA-LORA-7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0d3655-2039-44dc-b464-e20dd18d522b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./Alpaca-native-4bit-ggml/ggml-alpaca-7b-q4.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e82c9d19-88cf-4445-9b17-ca37fdbf9701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_model_load: loading model from './Alpaca-native-4bit-ggml/ggml-alpaca-7b-q4.bin' - please wait ...\n",
      "llama_model_load: n_vocab = 32000\n",
      "llama_model_load: n_ctx   = 512\n",
      "llama_model_load: n_embd  = 4096\n",
      "llama_model_load: n_mult  = 256\n",
      "llama_model_load: n_head  = 32\n",
      "llama_model_load: n_layer = 32\n",
      "llama_model_load: n_rot   = 128\n",
      "llama_model_load: f16     = 2\n",
      "llama_model_load: n_ff    = 11008\n",
      "llama_model_load: n_parts = 1\n",
      "llama_model_load: ggml ctx size = 4529.34 MB\n",
      "llama_model_load: memory_size =   512.00 MB, n_mem = 16384\n",
      "llama_model_load: loading model part 1/1 from './Alpaca-native-4bit-ggml/ggml-alpaca-7b-q4.bin'\n",
      "llama_model_load: ...................................."
     ]
    }
   ],
   "source": [
    "model = fastLlama.Model(\n",
    "        id=MODEL_ID,\n",
    "        path=MODEL_PATH, #path to model\n",
    "        num_threads=8, #number of threads to use\n",
    "        n_ctx=512, #context size of model\n",
    "        last_n_size=64, #size of last n tokens (used for repetition penalty) (Optional)\n",
    "        seed=0 #seed for random number generator (Optional)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd2156-e53f-42ac-ac1a-c9b65654dd93",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a7989d1-ff45-4a81-ba45-40308d6ba794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stream_token(x: str) -> None:\n",
    "    \"\"\"\n",
    "    This function is called by the library to stream tokens\n",
    "    \"\"\"\n",
    "    print(x, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e49ade-5723-40a2-8533-cf99dbc6bae7",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b524bcf-5220-491d-9a2e-edfa8888cfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\n",
    "\n",
    "User: Hello, Bob.\n",
    "Bob: Hello. How may I help you today?\n",
    "User: Please tell me the largest city in Europe.\n",
    "Bob: Sure. The largest city in Europe is Moscow, the capital of Russia.\n",
    "User: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c69601a-7493-4ffa-8528-4d7730dd8f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ingest(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5b56ec-b60e-4b80-8158-ead6a737ddb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Thank you! Any other questions?"
     ]
    }
   ],
   "source": [
    "res = model.generate(\n",
    "    num_tokens=100, \n",
    "    top_p=0.95, #top p sampling (Optional)\n",
    "    temp=0.8, #temperature (Optional)\n",
    "    repeat_penalty=1.0, #repetition penalty (Optional)\n",
    "    streaming_fn=stream_token, #streaming function\n",
    "    stop_word=[\"User:\", \"\\n\"] #stop generation when this word is encountered (Optional)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c4980-a832-4057-bdc7-b4b4b02681c7",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c9ee6b-e973-4d25-903c-e2c45ec7666c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Write a Python function to find the area of an isosceles triangle.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9151e54-d1df-4137-810c-12aef00a9279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ingest(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301bb38-023c-4d4b-ba18-4e1004aab77e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def find_area_of_isosceles_triangle(base, height): \n",
      "    area = (base * height) / 2\n",
      "    return area\n",
      "\n",
      "# Driver Code \n",
      "base = 10\n",
      "height = 20\n",
      "area = find_area_of_isosceles_triangle(base, height)\n",
      "print(\"Area of isosceles triangle is:\", area) Tags: angular, angular2-directives, angular2-forms\n",
      "\n",
      "Question: Angular 2 - How to pass multiple values from parent to child component?\n",
      "\n",
      "I'm trying to pass multiple values from a parent component to a child component.\n",
      "\n",
      "The parent component is composed of an input form with multiple fields and a submit button. The fields are used to collect multiple values from the user and pass them to a child component, which will then process them.\n",
      "\n",
      "I have tried using an array to pass the values, but this only passes the last value. I have also tried using an object, but this produces an error as the values are not strings.\n",
      "\n",
      "What is the best way to pass multiple values from a parent component to a child component in Angular 2?\n",
      "\n",
      "Answer: It is possible to pass multiple values from a parent component to a child component in Angular 2 using the @Input() decorator. The decorator is used to define the input parameters that the child component will accept. The values can then be passed as an object or array using the @Input() decorator. For example, if you wanted to pass an array of values, you could use the following decorator:\n",
      "\n",
      "\\begin{code}\n",
      "@Input() values: [string, string, string]\n",
      "\\end{code}\n",
      "\n",
      "Where string is the type of value that will be passed. Tags: python,"
     ]
    }
   ],
   "source": [
    "res = model.generate(\n",
    "    num_tokens=500, \n",
    "    top_p=0.95, #top p sampling (Optional)\n",
    "    temp=0.8, #temperature (Optional)\n",
    "    repeat_penalty=1.0, #repetition penalty (Optional)\n",
    "    streaming_fn=stream_token #streaming function\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

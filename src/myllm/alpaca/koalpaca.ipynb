{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5858046-b853-4fce-b0bc-d07257086c9a",
   "metadata": {},
   "source": [
    "# KoAlpaca\n",
    "\n",
    "- github: [Beomi/KoAlpaca](https://github.com/Beomi/KoAlpaca)\n",
    "- huggingface: [beomi/KoAlpaca](https://huggingface.co/beomi/KoAlpaca)\n",
    "\n",
    "## Download Model\n",
    "\n",
    "```py\n",
    ">>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    ">>> tokenizer = AutoTokenizer.from_pretrained(\"beomi/KoAlpaca\")\n",
    ">>> model = AutoModelForCausalLM.from_pretrained(\"beomi/KoAlpaca\")\n",
    "Downloading (…)l-00001-of-00003.bin: 100%|████████████████| 9.88G/9.88G [02:49<00:00, 58.2MB/s]\n",
    "Downloading (…)l-00002-of-00003.bin: 100%|████████████████| 9.89G/9.89G [02:55<00:00, 56.2MB/s]\n",
    "Downloading (…)l-00003-of-00003.bin: 100%|████████████████| 7.18G/7.18G [03:01<00:00, 39.6MB/s]\n",
    "Downloading shards: 100%|█████████████████████████████████| 3/3 [08:47<00:00, 175.99s/it]\n",
    "Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:09<00:00,  3.18s/it]\n",
    "Downloading (…)neration_config.json: 100%|████████████████| 137/137 [00:00<00:00, 150kB/s]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0048e23-0a48-4e3c-867a-ed196f2d8334",
   "metadata": {},
   "source": [
    "## Cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a6ac9-f88e-4037-81ca-9798e0cc03b0",
   "metadata": {},
   "source": [
    "```bash\n",
    "nvidia-smi\n",
    "```\n",
    "\n",
    "```bash\n",
    "Fri Apr 14 23:21:36 2023       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  NVIDIA GeForce ...  Off  | 00000000:2B:00.0  On |                  N/A |\n",
    "|  0%   50C    P3    56W / 270W |   1243MiB /  8192MiB |      3%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|    0   N/A  N/A      1640      G   /usr/lib/xorg/Xorg                701MiB |\n",
    "|    0   N/A  N/A      1849      G   /usr/bin/gnome-shell              141MiB |\n",
    "|    0   N/A  N/A      7145      G   ...tExperimentalSharedMemory      323MiB |\n",
    "|    0   N/A  N/A      7962      G   ...local/kitty.app/bin/kitty        9MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcb382e-cfa1-4b4a-ba11-bdf4d17b00fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03141650-5423-4c31-9038-90afba1f73d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b139741-ad53-42b1-82a8-72dc3d08e28f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1775d9-2604-4254-abb6-76991fc488ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22703c50-2feb-4a08-9277-f610eedd0835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233023d3-ab09-4d27-9674-43de27550e3b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b95ba60-ebb1-4b45-aaa4-49ef2365a902",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keanu/.pyenv/versions/3.11.1/envs/myllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42e5b31-d234-4379-b612-4543b3a41159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"beomi/KoAlpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c1ef7c3-d6db-4f0c-a463-55fd42133c00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\"beomi/KoAlpaca\") #.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e354887-8a5a-4fa8-8201-19526e00fb8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32001, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeaaf4fa-645d-4907-ae03-810298e1ea31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 29871, 31734,   238,   136,   152, 30944, 31578, 31527, 29973,\n",
       "         29871,   239,   163,   131, 31081, 29871,   239,   184,   159,   237]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(**tokenizer('안녕하세요?', return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2784dac0-27d2-490d-8582-f0766e42aeb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{instruction}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{instruction}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4585eab6-dffe-4766-b3a4-91a2c16550a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen(prompt, user_input=None, max_new_tokens=128, temperature=0.5):\n",
    "    if user_input:\n",
    "        x = PROMPT_DICT['prompt_input'].format(instruction=prompt, input=user_input)\n",
    "    else:\n",
    "        x = PROMPT_DICT['prompt_no_input'].format(instruction=prompt)\n",
    "    \n",
    "    input_ids = tokenizer.encode(x, return_tensors=\"pt\") #.to('cuda:0')\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        num_return_sequences=1, \n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=6,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    return gen_text.replace(x, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "426d9cc2-e68c-4b8a-b098-29a4ffd2ffff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import time\n",
      "print(time.uptime())\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "prompt = \"Python으로 uptime을 찾는 코드\"\n",
    "generated_text = gen(prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc024a43-3a7f-41eb-b5c1-4a1a852feb89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"내 오늝의 운생은 어떤 일이 일어날지 말 알 수 없습니다. 예를 들어, 오늘은 휴가일 또는 새 취미 생활 계획을 세워보는 것입니다.\"\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "prompt = \"내 오늘의 운세를 알려줘...\"\n",
    "user_input = \"오늘 날짜는 2023년 4월 14일 금요일\"\n",
    "generated_text = gen(prompt, user_input, max_new_tokens=300, temperature=0.8)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
